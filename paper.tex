\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{url}
\geometry{a4paper, margin=1in}

\title{\textbf{Analysis of the Muon Optimizer in a Transformer Mixture-of-Experts Language Model}}
\author{
    Vuk Rosić\textsuperscript{1,2}, [To Be Determined] \\
    \textsuperscript{1}Open Superintelligence Lab \\
    \textsuperscript{2}Óbuda University
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper documents a series of experiments analyzing the Muon optimizer for training a Mixture-of-Experts (MoE) Large Language Model. As part of an ongoing research effort, we conduct three primary experiments: a direct baseline comparison against the AdamW optimizer, an ablation study of Muon's core components (Momentum and Newton-Schulz orthogonalization), and a hyperparameter sensitivity analysis. We present the preliminary results from these experiments, which characterize the optimizer's behavior under various conditions. This work is part of an active research project, and final conclusions are yet to be drawn. The aim is to provide a transparent account of our methodology and current findings.
\end{abstract}

\vspace{1em}
\noindent\href{https://github.com/vukrosic/analysis-of-muon-optimizer-in-llms}{\textbf{GitHub Repository}}

\section{Introduction}
The field of Large Language Models (LLMs) has been dominated by optimizers from the Adam family, particularly AdamW \cite{loshchilov2017decoupled, kingma2014adam}. While effective, these methods can face challenges in stability and efficiency when scaling to increasingly large models. The Muon optimizer has emerged as a promising alternative, leveraging matrix-aware techniques, specifically Newton-Schulz orthogonalization, to potentially offer a more stable and efficient training process \cite{bernstein2020metricizing, liu2025muon, jordan2024muon}.

This paper investigates the practical performance of the Muon optimizer on a Mixture-of-Experts (MoE) transformer-based LLM. We aim to answer the following research questions:
\begin{itemize}
    \item How does a hybrid Muon optimizer compare to the standard AdamW optimizer in terms of performance and computational cost?
    \item What are the individual contributions of Muon's core components—momentum and Newton-Schulz orthogonalization?
    \item How sensitive is the Muon optimizer to its key hyperparameters?
\end{itemize}

To address these questions, we conduct a series of controlled experiments on a consistent model architecture and dataset.

\section{Background}
\subsection{Mixture-of-Experts (MoE) Models}
MoE models utilize a sparse activation strategy where only a fraction of the model's parameters are used for any given input \cite{shazeer2017outrageously}. This is achieved through a "router" network that directs input tokens to a subset of "expert" networks. This architecture allows for a significant increase in model capacity without a proportional increase in computational cost per forward pass, making it an efficient choice for large-scale models.

\subsection{The Muon Optimizer}
The Muon optimizer, short for MomentUm Orthogonalized by Newton-schulz, operates on entire matrices of parameters. Its key innovation is the use of the Newton-Schulz iteration to efficiently compute the orthogonalization of the gradient matrix \cite{schulz1933iteratives}. This process helps to maintain gradient stability and encourages a more uniform learning process across the network's layers. In our experiments, we employ a hybrid strategy where Muon is applied to the 2D weight matrices of the model, while AdamW is used for other parameters like embeddings and normalization layers.

The core update step involves:
\begin{enumerate}
    \item Applying momentum to the gradient.
    \item Orthogonalizing the resulting gradient using the \texttt{zeropower\_via\_newtonschulz5} function.
    \item Applying the final update to the parameter weights.
\end{enumerate}

\section{Experimental Setup}
All experiments were conducted using a consistent setup to ensure comparability of results.

\begin{itemize}
    \item \textbf{Model Architecture}: A 6-layer MoE Transformer with a model dimension of 384, 8 attention heads, and a feed-forward dimension of 1536. The model included 8 experts, with a top-2 routing strategy.
    \item \textbf{Dataset}: A 500,000 token subset of the SmolLM corpus.
    \item \textbf{Training}: All models were trained for 1000 steps with a batch size of 24 and 4 gradient accumulation steps.
    \item \textbf{Hardware}: Experiments were conducted in a low-compute environment, utilizing a single NVIDIA RTX 4090 GPU or a Google Colab T4 GPU. Future experiments may be upgraded to higher-compute environments.
\end{itemize}

\section{Results and Analysis}

\subsection{Experiment 1: Baseline Comparison (Muon vs. AdamW)}
This experiment compared the performance of the hybrid Muon optimizer against a pure AdamW optimizer.

\begin{table}[h!]
\centering
\caption{Experiment 1: Muon vs. AdamW Performance.}
\label{tab:exp1}
\begin{tabular}{@{}lcccc@{}}
\toprule
Metric & Muon & AdamW & Difference \\ \midrule
Validation Loss & \textbf{0.0476} & 0.0547 & -0.0072 \\
Validation Accuracy & \textbf{0.9907} & 0.9881 & +0.0026 \\
Validation Perplexity & \textbf{1.05} & 1.06 & -0.01 \\
Training Time (min) & 13.3 & \textbf{11.8} & +1.5 \\ \bottomrule
\end{tabular}
\end{table}

The results in Table \ref{tab:exp1} show that the Muon optimizer achieved a slightly better validation loss, accuracy, and perplexity. However, this came at the cost of a 12.7\% increase in training time compared to AdamW.

\subsection{Experiment 2: Ablation Study}
This experiment analyzed the contribution of Muon's two main components: momentum and Newton-Schulz (NS) orthogonalization.

\begin{table}[h!]
\centering
\caption{Experiment 2: Ablation Study Results.}
\label{tab:exp2}
\begin{tabular}{@{}lcccc@{}}
\toprule
Variant & Val Loss & Val Acc & Val PPL & Time (min) \\ \midrule
Full Muon (Momentum + NS) & \textbf{2.5347} & \textbf{0.4948} & \textbf{12.61} & 2.7 \\
Momentum Only (No NS) & 5.4336 & 0.1385 & 228.98 & \textbf{2.4} \\
NS Only (No Momentum) & 3.8273 & 0.2926 & 45.94 & 2.7 \\
Basic SGD-like (No Both) & 5.2608 & 0.1628 & 192.63 & \textbf{2.4} \\ \bottomrule
\end{tabular}
\end{table}

The ablation study (Table \ref{tab:exp2}) clearly demonstrates that both components are crucial. Removing either one led to a significant degradation in performance. The analysis indicated a strong, positive synergy effect of 1.4654, suggesting the components are more effective together than the sum of their individual contributions. Momentum appears to be the more critical component, as its removal resulted in a larger performance drop than the removal of Newton-Schulz.

\subsection{Experiment 3: Hyperparameter Sensitivity}
This experiment tested Muon's sensitivity to learning rate, momentum, and the number of Newton-Schulz steps.

\begin{table}[h!]
\centering
\caption{Experiment 3: Optimal Hyperparameters.}
\label{tab:exp3}
\begin{tabular}{@{}lc@{}}
\toprule
Hyperparameter & Optimal Value (based on Val Loss) \\ \midrule
Learning Rate & 0.05 (Loss: 0.3277) \\
Momentum & 0.95 (Loss: 2.5296) \\
Newton-Schulz Steps & 7 (Loss: 2.4955) \\ \bottomrule
\end{tabular}
\end{table}

The optimizer showed the highest sensitivity to the \textbf{learning rate}, with performance improving by 18.6x from the worst to the best-tested value. A higher learning rate of 0.05 was found to be optimal. The model was moderately sensitive to \textbf{momentum}, with 0.95 being the optimal value and 0.99 leading to degraded performance. Sensitivity to the number of \textbf{Newton-Schulz steps} was the weakest, with 7 steps performing best, but showing diminishing returns compared to 5 steps.

\section{Summary of Results and Future Work}
The experiments conducted provide preliminary data on the Muon optimizer's behavior. This research is ongoing, and the results presented here form the basis for further investigation.

\subsection{Summary of Current Results}
\begin{itemize}
    \item \textbf{Baseline Comparison}: The hybrid Muon optimizer achieved a marginally lower validation loss (0.0476) compared to AdamW (0.0547). This was accompanied by a 12.7\% increase in training time.
    \item \textbf{Ablation Study}: Both momentum and Newton-Schulz orthogonalization were observed to be integral to the optimizer's performance. The removal of either component resulted in a significant increase in validation loss. The results also point towards a synergistic relationship between the two components.
    \item \textbf{Hyperparameter Sensitivity}: The optimizer's performance is highly sensitive to the learning rate, with an optimal value found at 0.05 within the tested range. It showed moderate sensitivity to momentum (0.95 optimal) and weak sensitivity to the number of Newton-Schulz steps (7 optimal).
\end{itemize}

\subsection{Future Work}
This research is active and continues to evolve. Future experiments planned include:
\begin{itemize}
    \item A comprehensive learning rate sweep for both Muon and AdamW to visualize their sensitivity curves and identify optimal operating ranges.
    \item A detailed computational profiling analysis to precisely quantify the overhead of the Newton-Schulz iteration and identify potential optimization bottlenecks.
    \item Scaling experiments on larger models and datasets to determine if the observed performance characteristics hold true for more complex training regimes.
    \item Investigation into the interaction between hyperparameters, potentially through a grid search, to find a globally optimal configuration.
\end{itemize}
Final conclusions will be drawn upon the completion of these and subsequent experiments.

\begin{thebibliography}{9}

    \bibitem{loshchilov2017decoupled}
    Ilya Loshchilov and Frank Hutter.
    \textit{Decoupled Weight Decay Regularization}.
    International Conference on Learning Representations (ICLR), 2019.  
    \href{https://arxiv.org/abs/1711.05101}{arXiv:1711.05101}
    
    \bibitem{bernstein2020metricizing}
    Jeremy Bernstein, et al.
    \textit{On the Distance Between Two Neural Networks and the Stability of Learning}.  
    \href{https://arxiv.org/abs/2002.03432}{arXiv:2002.03432}, 2020.
    
    \bibitem{kingma2014adam}
    Diederik P. Kingma and Jimmy Ba.  
    \textit{Adam: A Method for Stochastic Optimization}.  
    \href{https://arxiv.org/abs/1412.6980}{arXiv:1412.6980}, 2014.
    
    \bibitem{shazeer2017outrageously}
    Noam Shazeer, et al.  
    \textit{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}.  
    \href{https://arxiv.org/abs/1701.06538}{arXiv:1701.06538}, 2017.
    
    \bibitem{liu2025muon}
    Jingyuan Liu, et al.  
    \textit{Muon is Scalable for LLM Training}.  
    \href{https://arxiv.org/abs/2502.16982}{arXiv:2502.16982}, 2025.
    
    \bibitem{jordan2024muon}
    Keller Jordan.  
    \textit{Muon: An optimizer for hidden layers in neural networks}.  
    \url{https://kellerjordan.github.io/posts/muon/}, 2024.
    
    \bibitem{schulz1933iteratives}
    G. Schulz.  
    \textit{Iterative Berechnung der Reziproken Matrix}.  
    Zeitschrift für Angewandte Mathematik und Mechanik, 13:57–59, 1933.
    
    \end{thebibliography}
    

\end{document}
